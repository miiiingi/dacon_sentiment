{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miiiingi/dacon_sentiment/blob/main/dacon_sentimental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hwyVFOIFs-4",
        "outputId": "161c4b83-2338-4a36-91a1-466eee4d8950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n",
        "my_folder = '/gdrive/MyDrive/ColabNotebooks/dacon_senti/dataset/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctYxYX8XGDop",
        "outputId": "86ef6116-7bcf-41dc-c24d-4b034283ccb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Mecab\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from torchtext.legacy import data, datasets\n",
        "import torch\n",
        "import random\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "SEED = 722\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2i6MOY0jL0y",
        "outputId": "0d694de7-f348-46b1-e80d-f462ce175dd4"
      },
      "execution_count": 323,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 109 (delta 7), reused 10 (delta 3), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (109/109), 1.27 MiB | 8.33 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "/content/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2022-01-19 08:56:18--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::3403:4be7, 2406:da00:ff00::22e9:9f55, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=s3OvGqVgCPU4WQOqm0ToQvmGW8Y%3D&Expires=1642583866&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2022-01-19 08:56:19--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=s3OvGqVgCPU4WQOqm0ToQvmGW8Y%3D&Expires=1642583866&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.240.68\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.240.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  2.73MB/s    in 0.5s    \n",
            "\n",
            "2022-01-19 08:56:20 (2.73 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2022-01-19 08:57:51--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22cd:e0db, 2406:da00:ff00::22c5:2ef4, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=jc25DM%2BiuIrf%2FOBOpatKTwdqiWk%3D&Expires=1642583678&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2022-01-19 08:57:51--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=jc25DM%2BiuIrf%2FOBOpatKTwdqiWk%3D&Expires=1642583678&AWSAccessKeyId=AKIA6KOSE3BNA7WTAGHW&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.41.12\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.41.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  22.3MB/s    in 2.1s    \n",
            "\n",
            "2022-01-19 08:57:54 (22.3 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTuFGseES5oO"
      },
      "source": [
        "Torchtext - Field 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eJmv_1LMRmq9"
      },
      "outputs": [],
      "source": [
        "tokenizer = Mecab()\n",
        "ID = data.Field(sequential=False, use_vocab=False)\n",
        "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.morphs, lower=True, batch_first=True)\n",
        "LABEL = data.LabelField(sequential=False, use_vocab=False, is_target=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째 실험 : tokenizer을 morphs(75.37 / clip grad norm x > 과적합 심하게 발생) > nouns(68.4)로 변화 / morphs \\\\\n",
        "두 번째 실험 : clip grad norm(73.9 > 과적합 발생)\n"
      ],
      "metadata": {
        "id": "UbyEAerP6F1N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k48eaLrAS_Lr"
      },
      "source": [
        "Torchtext - 데이터셋 불러와서 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d28sGlSZOfbT"
      },
      "outputs": [],
      "source": [
        "train_data = data.TabularDataset(\n",
        "                            path = f'{my_folder}/train.csv',\n",
        "                            format = 'csv',\n",
        "                            fields = [('id', ID), ('document', TEXT), ('label', LABEL)],\n",
        "                            skip_header = True,)\n",
        "test_data = data.TabularDataset(\n",
        "                            path = f'{my_folder}/test.csv',\n",
        "                            format = 'csv',\n",
        "                            fields = [('id', ID), ('document', TEXT)],\n",
        "                            skip_header = True,)\n",
        "train_data, valid_data = train_data.split(split_ratio = 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sDy8pfLTOXY"
      },
      "source": [
        "Torchtext - 단어 집합 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CrRJHNBdESTd"
      },
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train_data, min_freq=2, max_size= 1000) # 단어 집합 생성\n",
        "# LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars(train_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIXRm1iq3CZ5",
        "outputId": "8ffb1a59-e942-49e7-ab3e-4ce9edc0558a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': ['우리', '나라', '좀', '비', '영화', '는', '왜', '이러', '냐', '.', '.'],\n",
              " 'id': '1494',\n",
              " 'label': '0'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tUAImEsUFIB"
      },
      "source": [
        "Torchtext - 배치화 시키기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_loader = data.Iterator(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = data.Iterator(valid_data, batch_size=batch_size, train=False, sort=False)\n",
        "test_loader = data.Iterator(test_data, batch_size=batch_size, train=False, sort=False)"
      ],
      "metadata": {
        "id": "XsSbAdZKYiJR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(next(iter(train_loader)))\n",
        "print(next(iter(val_loader)))\n",
        "print(next(iter(test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3EiI96ciQPl",
        "outputId": "7daf0911-7f1f-418d-a4f6-65e98a2e89c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 64]\n",
            "\t[.id]:[torch.LongTensor of size 64]\n",
            "\t[.document]:[torch.LongTensor of size 64x25]\n",
            "\t[.label]:[torch.LongTensor of size 64]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 64]\n",
            "\t[.id]:[torch.LongTensor of size 64]\n",
            "\t[.document]:[torch.LongTensor of size 64x26]\n",
            "\t[.label]:[torch.LongTensor of size 64]\n",
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 64]\n",
            "\t[.id]:[torch.LongTensor of size 64]\n",
            "\t[.document]:[torch.LongTensor of size 64x23]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer 어떻게 처리할 지 생각해보자"
      ],
      "metadata": {
        "id": "7_isE2ellRQv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pOhHmRMU_3G"
      },
      "source": [
        "GRU Model 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "ldD8b2O8UTJ1"
      },
      "outputs": [],
      "source": [
        "from torch import nn as nn \n",
        "import torch.nn.functional as F\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(GRU, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers * 2,\n",
        "                          batch_first=True,\n",
        "                          bidirectional = True)\n",
        "        self.out = nn.Linear(self.hidden_dim * 2, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(x.shape)\n",
        "        x = self.embed(x)\n",
        "        # print(x.shape)\n",
        "        # h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화\n",
        "        x, _ = self.gru(x)  # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n",
        "        h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n",
        "        self.dropout(h_t)\n",
        "        logit = self.out(h_t)  # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n",
        "        return logit\n",
        "\n",
        "    def _init_state(self, batch_size=1):\n",
        "        weight = next(self.parameters()).data\n",
        "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        import math\n",
        "        super(PositionalEncoding, self).__init__()       \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        even_div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        odd_div_term = torch.exp(torch.arange(1, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * even_div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * odd_div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        pe.requires_grad = False\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed_pre, n_embed_post, n_layers, n_head, n_linear, n_classes, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_mask = None\n",
        "        # self.embed = nn.Embedding(n_vocab, n_embed_post)\n",
        "        self.pos_enc = PositionalEncoding(n_embed_pre) # 나중에 비교해보자 인코딩 방법\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=n_embed_post, nhead = n_head, dropout=dropout, batch_first = True)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=n_layers)        \n",
        "        self.linear = nn.Linear(n_embed_post, n_classes)\n",
        "        # self.init_weights()\n",
        "    # def init_weights(self):\n",
        "    #     nn.init.zeros_(self.decoder.bias)\n",
        "    #     nn.init.zeros_(self.decoder2.bias)\n",
        "    #     nn.init.xavier_normal_(self.decoder.weight)\n",
        "    #     nn.init.xavier_normal_(self.decoder2.weight)\n",
        "    def forward(self, src):\n",
        "        print(src.shape)\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            self.src_mask = self._generate_square_subsequent_mask(src.shape[1]).to(src.device)\n",
        "        # print(src.shape)\n",
        "        # src = self.embed(src)\n",
        "        # print(src.shape)\n",
        "        src = self.pos_enc(src)\n",
        "        # print(src.shape)\n",
        "        exit()\n",
        "        # print(self.src_mask.shape)\n",
        "        output = self.encoder(src, self.src_mask)\n",
        "        # print(output.shape)\n",
        "        output = self.linear(output)\n",
        "        # position = src[:, 0, :]\n",
        "        # src_original = torch.transpose(src_original, 0, 1)\n",
        "        # src = torch.transpose(src, 0, 1)\n",
        "        # feature_time = torch.transpose(feature_time, 0, 1)\n",
        "        # output, attention = self.transformer_encoder(src, self.src_mask)\n",
        "        # if self.type_enc == 'vanila' : \n",
        "        #     output = torch.cat((output, feature_time), 2)\n",
        "        # elif self.type_enc != 'vanila' : \n",
        "        #     output = output \n",
        "        # output = self.decoder(output)\n",
        "        # output = self.decoder2(output)\n",
        "        # return output, attention, position \n",
        "        return output\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ],
      "metadata": {
        "id": "CZs_2_KyHpiI"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "giqPHsMPVS5p"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = GRU(1, 256, len(TEXT.vocab), 128, 2, 0.3).to(DEVICE)\n",
        "model = Transformer(n_vocab = len(TEXT.vocab),n_embed_pre = 25, n_embed_post = 128, n_layers = 1, n_head = 8, n_linear = 128, n_classes = 2, dropout = 0.3).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "id": "0vjeZhRHgTnp"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, train_iter):\n",
        "    model.train()\n",
        "    for b, batch in enumerate(train_iter):\n",
        "        x, y = batch.document.to(DEVICE), batch.label.long().to(DEVICE)\n",
        "        print(x.shape)\n",
        "        print(y.shape)\n",
        "        print(x)\n",
        "        optimizer.zero_grad()\n",
        "        print(model(x).shape)\n",
        "\n",
        "        logit = torch.argmax(model(x), dim = 2)\n",
        "        print(logit.shape)\n",
        "        print(y.shape)\n",
        "        loss = F.cross_entropy(logit, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "id": "32uYA2hzaH_w"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_iter):\n",
        "    \"\"\"evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    corrects, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "      for batch in val_iter:\n",
        "          x, y = batch.document.to(DEVICE), batch.label.to(DEVICE)\n",
        "          logit = model(x)\n",
        "          loss = F.cross_entropy(logit, y, reduction='sum')\n",
        "          total_loss += loss.item()\n",
        "          corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
        "      size = len(val_iter.dataset)\n",
        "      avg_loss = total_loss / size\n",
        "      avg_accuracy = 100.0 * corrects / size\n",
        "      return avg_loss, avg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test(model, test_loader, device):\n",
        "    result = []\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            x = batch.document.to(device)\n",
        "            out = model(x)\n",
        "            pred = torch.argmax(out, dim=1)\n",
        "            result.append(pred)\n",
        "    return torch.cat(result).cpu().numpy()"
      ],
      "metadata": {
        "id": "q8Sfqp8adEgT"
      },
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "pr4o2MwHZ3SG",
        "outputId": "7743d224-7946-4b52-98ba-2149c108f6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 22])\n",
            "torch.Size([64])\n",
            "tensor([[  0,   0, 120,  ...,   1,   1,   1],\n",
            "        [  0,   0,   0,  ...,   1,   1,   1],\n",
            "        [  0,   0, 205,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [279, 279,  46,  ...,   1,   1,   1],\n",
            "        [ 23,   0,  22,  ...,   1,   1,   1],\n",
            "        [284,   3, 208,  ...,   1,   1,   1]], device='cuda:0')\n",
            "torch.Size([64, 22])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-310-f21db6546fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mEpochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-307-25839522abde>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-301-329452e15f67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# src = self.embed(src)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# print(src.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# print(src.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-301-329452e15f67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (22) must match the size of tensor b (25) at non-singleton dimension 2"
          ]
        }
      ],
      "source": [
        "best_val_loss = None\n",
        "val_accuracy_accum = 0\n",
        "Epochs = 1\n",
        "for e in tqdm(range(Epochs)):\n",
        "    train(model, optimizer, train_loader)\n",
        "    train_loss, train_accuracy = evaluate(model, train_loader)\n",
        "    val_loss, val_accuracy = evaluate(model, val_loader)\n",
        "    val_accuracy_accum += val_accuracy\n",
        "\n",
        "    print(\"[Epoch: %d] train loss : %5.2f | train acc : %5.2f | val loss : %5.2f | val accuracy : %5.2f\" % (e+1, train_loss, train_accuracy, val_loss, val_accuracy))\n",
        "\n",
        "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        if not os.path.isdir(\"/gdrive/MyDrive/ColabNotebooks/dacon_senti/snapshot\"):\n",
        "            os.makedirs(\"/gdrive/MyDrive/ColabNotebooks/dacon_senti/snapshot\")\n",
        "        torch.save(model.state_dict(), f'/gdrive/MyDrive/ColabNotebooks/dacon_senti/snapshot/sentiment.pt')\n",
        "        best_val_loss = val_loss\n",
        "print(f'final validation score : {val_accuracy_accum / Epochs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST 진행"
      ],
      "metadata": {
        "id": "Bnau_wbvgVU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcRzc9RgaVfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31eed290-a8f7-4b38-ec6b-0d00d5ef1e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:01<00:00, 76.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 ... 1 1 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(f'/gdrive/MyDrive/ColabNotebooks/dacon_senti/snapshot/sentiment.pt'))\n",
        "result = evaluate_test(model, test_loader, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(f\"{my_folder}/sample_submission.csv\")\n",
        "submission[\"label\"] = result\n",
        "print(submission)\n",
        "submission.to_csv(f\"{my_folder}/submission.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2l0O-lzgS2m",
        "outputId": "2a982634-aece-4c30-f2b1-4b824583bce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id  label\n",
            "0        1      0\n",
            "1        2      1\n",
            "2        3      0\n",
            "3        4      1\n",
            "4        5      0\n",
            "...    ...    ...\n",
            "4995  4996      0\n",
            "4996  4997      0\n",
            "4997  4998      1\n",
            "4998  4999      1\n",
            "4999  5000      1\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6u7GxxFQbIy4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dacon_sentimental analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPxdpnrsV1oKBbyNFw5vfU7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}