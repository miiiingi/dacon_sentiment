{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon_sentimental analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNchxhbeCCZsLddKPp4mOZ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miiiingi/dacon_sentiment/blob/main/dacon_sentimental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hwyVFOIFs-4",
        "outputId": "907ed2d5-b91e-42eb-a37f-a50a56fc4509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n",
        "my_folder = '/gdrive/MyDrive/ColabNotebooks/dacon_senti/dataset/dataset'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "trainset = pd.read_csv(f'{my_folder}/train.csv')\n",
        "valset = trainset.iloc[int(len(trainset) * 0.5):, :]\n",
        "trainset = trainset.iloc[: int(len(trainset) * 0.5), :]\n",
        "testset = pd.read_csv(f'{my_folder}/test.csv')"
      ],
      "metadata": {
        "id": "ctYxYX8XGDop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ff8c946-6a1f-4977-bfa9-9ea34553fc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 40.3 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 42.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터셋 전처리"
      ],
      "metadata": {
        "id": "74Q3ksqSKhGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset['document'] = trainset['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "trainset['document'] = trainset['document'].str.replace('^ +', \"\")\n",
        "trainset['document'].replace('', np.nan, inplace=True)\n",
        "trainset = trainset.dropna(how = 'any')"
      ],
      "metadata": {
        "id": "x9v3vNKkGnxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb38990-a2bf-4496-f8fb-83a08a50be60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                               document  label\n",
            "0        1              영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐      0\n",
            "1        2              히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯      1\n",
            "2        3       괜찮은 음악영화가 또 나왔군요 따뜻한 겨울이 될 것 같아요      1\n",
            "3        4              아무래도 년도지난작품이라 지금보기는너무유치하다      0\n",
            "4        5  지금까지의 영화들이 그랬듯 이 영화역시 일본에 대한 미화는 여전하다      0\n",
            "...    ...                                    ...    ...\n",
            "2495  2496             조디 포스터와 깁슨은 역시 대단하다 감탄 그자체      1\n",
            "2496  2497  편은편을위해 존재했다고보여질정도로 편은 상당한재미와 완성도를보여준다      1\n",
            "2497  2498               인생영화중 하나그리고 가슴을 파고드는 엔딩송      1\n",
            "2498  2499              뭐지 이건 뭘 이야기하고 싶은건지 평점너무높다      0\n",
            "2499  2500                시간만 때우자 했을때도 주저해야 할 드라마      0\n",
            "\n",
            "[2495 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "검증 데이터셋 전처리"
      ],
      "metadata": {
        "id": "KB8Ux-ibB4bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valset['document'] = valset['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "valset['document'] = valset['document'].str.replace('^ +', \"\")\n",
        "valset['document'].replace('', np.nan, inplace=True)\n",
        "valset = valset.dropna(how = 'any')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_vne2TkB7Gi",
        "outputId": "d8a1572b-3061-47b5-b5d2-d1d770d626b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                document  label\n",
            "2500  2501        쌍칼형님 연기 너무 못해서 정말 실망였어요 연기력 키우세요      0\n",
            "2501  2502     작품성은 있을지 몰라도 ㅡㅡ 이런 애들장난 보고싶은게 아니었는데      0\n",
            "2502  2503                아무리 애들영화라지만 스토리가 너무 성의없다      0\n",
            "2503  2504                        영화로 만드는 자체가 무리였어      0\n",
            "2504  2505                이 영화를 보고 새로운 삶을 살게 되었습니다      1\n",
            "...    ...                                     ...    ...\n",
            "4995  4996                 좋은 배우들로 류영화를 찍은 안타까운 영화      0\n",
            "4996  4997  진짜 드럽게 재미없다 에드워드 호퍼 그림에 배경 빼고는 볼게 아닌영화      0\n",
            "4997  4998                   가장 실망스러운 영화 지금까지 본영화중      0\n",
            "4998  4999           이런 평점 테러 네이버에서 좀 막아야 하는 것 아닌가      1\n",
            "4999  5000       여주인공이 전작에서는 귀여웠는데 여기선 완전 망했네 실망이다      0\n",
            "\n",
            "[2495 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터 전처리 > 특수 문자를 없애고 공백을 NaN으로 처리하니까 NaN값이 많이 생겼음.... >> 이것을 어떻게 처리 할 것 인가에 대해 생각해보자"
      ],
      "metadata": {
        "id": "lvUwHUSQLF_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testset['document'] = testset['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "testset['document'] = testset['document'].str.replace('^ +', \"\")\n",
        "testset['document'].replace('', np.nan, inplace=True)\n",
        "print(testset.loc[testset['document'].isnull() == True, :])\n",
        "testset = testset.dropna(how = 'any')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q00mm8xmKnVk",
        "outputId": "e7d810d9-4f29-4cfb-fc26-9a5569a28360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id document\n",
            "2        3      NaN\n",
            "458    459      NaN\n",
            "662    663      NaN\n",
            "756    757      NaN\n",
            "1023  1024      NaN\n",
            "1313  1314      NaN\n",
            "1325  1326      NaN\n",
            "1386  1387      NaN\n",
            "1392  1393      NaN\n",
            "2294  2295      NaN\n",
            "2599  2600      NaN\n",
            "3060  3061      NaN\n",
            "3473  3474      NaN\n",
            "3654  3655      NaN\n",
            "3857  3858      NaN\n",
            "3997  3998      NaN\n",
            "4001  4002      NaN\n",
            "4707  4708      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련 데이터셋에 대해 불용어 처리하고 토큰화 진행"
      ],
      "metadata": {
        "id": "mc-98MdAOlhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '이라', '까지의', '까지', '에서', '것', '게']\n",
        "okt = Okt()\n",
        "trainset_sentence = list()\n",
        "for sentence in tqdm(trainset['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    trainset_sentence.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d28sGlSZOfbT",
        "outputId": "cc440c3b-86c8-4577-c421-2e1a57d65599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2495/2495 [00:17<00:00, 143.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset_processed = pd.DataFrame({'document' : trainset_sentence, 'label' : trainset['label']})\n",
        "print(trainset_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBGCjdvyIxiT",
        "outputId": "20d8d9e7-4e1c-43dc-e779-52d1f2f18453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               document  label\n",
            "0              [영상, 이나, 음악, 이쁘다, 해도, 미화, 시키다, 불륜, 일, 뿐]      0\n",
            "1                             [히치콕, 영화, 보다, 분명, 박수, 치다]      1\n",
            "2               [괜찮다, 음악, 영화, 또, 나오다, 따뜻하다, 겨울, 되다, 같다]      1\n",
            "3                  [아무래도, 년도, 지난, 작품, 지금, 보기, 너무, 유치하다]      0\n",
            "4               [지금, 영화, 그렇다, 영화, 역시, 일본, 대한, 미화, 여전하다]      0\n",
            "...                                                 ...    ...\n",
            "2495                 [조디, 포스터, 깁슨, 역시, 대단하다, 감탄, 그, 자체]      1\n",
            "2496  [편, 편, 을, 위해, 존재, 보이다, 정도, 로, 편, 상당하다, 재미, 완성,...      1\n",
            "2497           [인생, 영화, 중, 하나, 그리고, 가슴, 을, 파고들다, 엔딩, 송]      1\n",
            "2498        [뭐, 지, 이건, 뭘, 이야기, 하고, 싶다, 건지다, 평점, 너무, 높다]      0\n",
            "2499                           [시간, 만, 때우다, 때, 주저, 드라마]      0\n",
            "\n",
            "[2495 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "검증 데이터셋에 대해 불용어 처리하고 토큰화 진행"
      ],
      "metadata": {
        "id": "t7_rz7PADKeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valset_sentence = list()\n",
        "okt = Okt()\n",
        "for sentence in tqdm(valset['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    valset_sentence.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR-UKJONDKGQ",
        "outputId": "15d917d7-a4fb-4888-825d-adef47190b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2495/2495 [00:06<00:00, 392.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valset_processed = pd.DataFrame({'document' : valset_sentence, 'label' : valset['label']})\n",
        "print(valset_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmXx8A1hDJ78",
        "outputId": "35b60b43-9939-4fec-c818-1d0a6733c9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               document  label\n",
            "2500          [쌍칼, 형님, 연기, 너무, 못, 정말, 실망, 이다, 연기력, 키우다]      0\n",
            "2501     [작품, 성은, 있다, 모르다, ㅡㅡ, 이렇다, 애, 장난, 보고, 싶다, 아니다]      0\n",
            "2502               [아무리, 애, 영화, 라지, 만, 스토리, 너무, 성의, 없다]      0\n",
            "2503                                [영화로, 만들다, 자체, 무리다]      0\n",
            "2504                        [영화, 보고, 새롭다, 삶, 을, 살, 되어다]      1\n",
            "...                                                 ...    ...\n",
            "4995                   [좋다, 배우, 로, 류, 영화, 찍다, 안타깝다, 영화]      0\n",
            "4996  [진짜, 드럽다, 재미없다, 에드워드, 호퍼, 그림, 배경, 빼다, 볼, 아니다, 영화]      0\n",
            "4997                    [가장, 실망, 스러운, 영화, 지금, 본, 영화, 중]      0\n",
            "4998                        [이렇다, 평점, 테러, 네이버, 막다, 아니다]      1\n",
            "4999     [여, 주인공, 전작, 에서는, 귀엽다, 여, 기선, 완전, 망하다, 실망, 이다]      0\n",
            "\n",
            "[2495 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset_sentence = list()\n",
        "for sentence in tqdm(testset['document']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    testset_sentence.append(stopwords_removed_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gksjrwBQPbHH",
        "outputId": "ef9e6772-9a09-4078-b901-ea0d6b8ed85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4982/4982 [00:11<00:00, 422.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset_processed = pd.DataFrame({'document' : testset_sentence})\n",
        "print(testset_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiFfOvTrQDn8",
        "outputId": "c7ae853c-cd3a-4b36-fb47-76d0e8fef389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            document\n",
            "0                             [시간, 때우다, 좋다, 영화, 지루함]\n",
            "1       [훈훈하다, 정이, 느껴지다, 영화, 가족, 끼리, 드라마, 보다, 보다, 딱]\n",
            "2                           [멋있다, 영화, 이다, 잊다, 수, 없다]\n",
            "3                               [너무, 감동, 적다, 펑펑, 울다]\n",
            "4     [어이, 김구라, 자세, 똑바로, 앉다, 방송, 마다, 왜케, 삐, 딱하다, 앉다]\n",
            "...                                              ...\n",
            "4977    [연, 계, 제대로, 안되다, 뭔가, 부족하다, 느낌, 들다, 찝찝하다, 영화]\n",
            "4978                [내생, 최악, 영화, 중, 하나, 졸리다, 미치다, 뻔]\n",
            "4979            [지금, 까지, 나오다, 드라마, 중, 최고, 이다, 점, 만점]\n",
            "4980              [영화, 값, 엄청나다, 공연, 을, 보다, 게, 웬, 횡재]\n",
            "4981                      [올해, 엔, 롯데, 자이언츠, 꼭, 우승하다]\n",
            "\n",
            "[4982 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "등장 단어 데이터 셋 만들기"
      ],
      "metadata": {
        "id": "uyB25ddsYn_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "wordset = collections.defaultdict(int)\n",
        "wordset_freq = collections.defaultdict(int)\n",
        "for sentence in tqdm(trainset_processed['document']) :\n",
        "  for word in sentence : \n",
        "    wordset[word] += 1\n",
        "for k, v in wordset.items() : \n",
        "  if wordset[k] > 5 :\n",
        "    wordset_freq[k] = v\n",
        "wordset['<unk>'] = 0\n",
        "wordset['<pad>'] = 1 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBRieOwhQOex",
        "outputId": "94b8bf77-0355-43b3-918e-19d22a617d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4990/4990 [00:00<00:00, 241307.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 임베딩 벡터 만들기"
      ],
      "metadata": {
        "id": "GS7dI0ewfj1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn as nn\n",
        "embed_layer = nn.Embedding(num_embeddings=len(wordset_freq),\n",
        "                           embedding_dim = 16,\n",
        "                           padding_idx = 1) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilSi_JobVFbT",
        "outputId": "b7399d52-4bd0-4fae-e5cc-7a58566ab05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.5725, -0.0116, -0.8020,  ..., -1.0997, -1.2119,  0.8811],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-1.7805,  0.0376, -1.4147,  ...,  0.8625,  1.0358,  1.2306],\n",
            "        ...,\n",
            "        [ 0.1353,  1.4251, -0.2256,  ...,  0.2817,  0.7274, -0.0612],\n",
            "        [ 0.1899,  0.0749, -0.2070,  ..., -2.5309, -0.6005,  0.6978],\n",
            "        [-0.2626,  1.8274,  0.1970,  ..., -0.9226, -1.8234, -1.7732]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 분리하기"
      ],
      "metadata": {
        "id": "SfvwCbK8fmjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import data\n",
        "print(dir(data))\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "        (trainset, valset, testset), batch_size=BATCH_SIZE,\n",
        "        shuffle=True, repeat=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "3Iz1m8jOegEX",
        "outputId": "74fe0b77-99b6-49da-c35f-08f7a9eaea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bleu_score', 'custom_replace', 'datasets_utils', 'filter_wikipedia_xml', 'functional', 'generate_sp_model', 'get_tokenizer', 'interleave_keys', 'load_sp_model', 'metrics', 'numericalize_tokens_from_iterator', 'sentencepiece_numericalizer', 'sentencepiece_tokenizer', 'simple_space_split', 'to_map_style_dataset', 'utils']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-716af0962e6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_iter, val_iter, test_iter = data.BucketIterator.splits(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         shuffle=True, repeat=False)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'BucketIterator'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0vjeZhRHgTnp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}